{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNy1rE2vtXLPfh18+INWO2V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreSlavescu/triton-exercises/blob/main/triton_tutorials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install triton torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLEmzmI2PFE6",
        "outputId": "4c8588e9-d2f8-4f14-f75f-cfdca25a56d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZg-HSAgO_qC",
        "outputId": "fe31a116-1af2-44b3-c332-e5f3a851d54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum difference between torch and triton is: 0.0\n",
            "mean ms: 0.010818585753440857\n"
          ]
        }
      ],
      "source": [
        "# Vector Addition\n",
        "\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import triton.testing as tt\n",
        "\n",
        "@triton.jit\n",
        "def add_kernel(\n",
        "    x,\n",
        "    y,\n",
        "    out,\n",
        "    n_elems,\n",
        "    BLOCK_SIZE: tl.constexpr\n",
        "):\n",
        "  pid = tl.program_id(axis=0)\n",
        "  block_start = pid * BLOCK_SIZE\n",
        "  offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "\n",
        "  mask = offsets < n_elems\n",
        "  x_tensor = tl.load(x + offsets, mask)\n",
        "  y_tensor = tl.load(y + offsets, mask)\n",
        "\n",
        "  out_tensor = tl.store(out + offsets, x_tensor + y_tensor, mask=mask)\n",
        "\n",
        "def add(\n",
        "    x: torch.Tensor,\n",
        "    y: torch.Tensor\n",
        "):\n",
        "  out = torch.empty_like(x)\n",
        "  assert x.is_cuda and y.is_cuda and out.is_cuda and x.numel() == y.numel()\n",
        "\n",
        "  n_elems = x.numel()\n",
        "\n",
        "  grid = lambda meta: (triton.cdiv(n_elems, meta['BLOCK_SIZE']), )\n",
        "  add_kernel[grid](x, y, out, n_elems, BLOCK_SIZE=1024)\n",
        "\n",
        "  return out\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  torch.manual_seed(0)\n",
        "  size = 98432\n",
        "  x = torch.rand(size, device='cuda')\n",
        "  y = torch.rand(size, device='cuda')\n",
        "  out_torch = x + y\n",
        "  out_triton = add(x, y)\n",
        "\n",
        "  print(f'The maximum difference between torch and triton is: '\n",
        "      f'{torch.max(torch.abs(out_torch - out_triton))}')\n",
        "\n",
        "  mean_ms = triton.testing.do_bench(lambda: add(x, y))\n",
        "  print(f\"mean ms: {mean_ms}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prefix Sum\n",
        "\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import triton.testing as tt\n",
        "\n",
        "import os\n",
        "os.environ['TRITON_PRINT_AUTOTUNING'] = \"1\"\n",
        "\n",
        "@triton.autotune(configs = [\n",
        "    triton.Config({'BLOCK_SIZE': 128}, num_warps = 4),\n",
        "    triton.Config({'BLOCK_SIZE': 1024}, num_warps = 8),\n",
        "], key = ['n_elems'])\n",
        "@triton.jit\n",
        "def prefix_sum_kernel(\n",
        "    x,\n",
        "    out,\n",
        "    n_elems,\n",
        "    BLOCK_SIZE: tl.constexpr\n",
        "):\n",
        "  pid = tl.program_id(axis=0)\n",
        "  block_start = pid * BLOCK_SIZE\n",
        "  offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "  tl.static_print(\"offsets: \", offsets)\n",
        "\n",
        "  mask = offsets < n_elems\n",
        "  x_tensor = tl.load(x + offsets, mask)\n",
        "\n",
        "  out_tensor = tl.store(out + offsets, x_tensor, mask=mask)\n",
        "\n",
        "def prefix_sum(\n",
        "    x: torch.Tensor\n",
        "):\n",
        "  out = torch.empty_like(x)\n",
        "  assert x.is_cuda and out.is_cuda\n",
        "\n",
        "  n_elems = x.numel()\n",
        "\n",
        "  grid = lambda meta: (triton.cdiv(n_elems, meta['BLOCK_SIZE']), )\n",
        "  prefix_sum_kernel[grid](x, out, n_elems, BLOCK_SIZE=1024)\n",
        "\n",
        "  return out\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  torch.manual_seed(0)\n",
        "  size = 98432\n",
        "  x = torch.rand(size, device='cuda')\n",
        "  out_torch = torch.cumsum(x, dim=0)\n",
        "  out_triton = prefix_sum(x)\n",
        "\n",
        "  print(f'The maximum difference between torch and triton is: '\n",
        "      f'{torch.max(torch.abs(out_torch - out_triton))}')\n",
        "\n",
        "  mean_ms = triton.testing.do_bench(lambda: prefix_sum(x))\n",
        "  print(f\"mean ms: {mean_ms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRSz6gnDZHYz",
        "outputId": "ed46937d-96b7-407e-bf65-36ab279a7e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "offsets:  int32[constexpr[1024]]\n",
            "The maximum difference between torch and triton is: 49189.1015625\n",
            "mean ms: 0.008661333471536636\n"
          ]
        }
      ]
    }
  ]
}